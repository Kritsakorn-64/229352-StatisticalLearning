{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kritsakorn-64/229352-StatisticalLearning/blob/main/%E0%B8%AA%E0%B8%B3%E0%B9%80%E0%B8%99%E0%B8%B2%E0%B8%82%E0%B8%AD%E0%B8%87_Lab10_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical Learning for Data Science 2 (229352)\n",
        "#### Instructor: Donlapark Ponnoprat\n",
        "\n",
        "#### [Course website](https://donlapark.pages.dev/229352/)\n",
        "\n",
        "## Lab #10"
      ],
      "metadata": {
        "id": "R3EHk_vLhxDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import torchvision # For utils.make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm # For nice progress bars\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Set device to GPU if available, else CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "t-jV4RoaQjuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff25da1-7fee-4703-ba99-20ca2506b928"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a dataset of pizza, stead and sushi images ([source](https://donlapark.pages.dev/229352/pizza_steak_sushi.zip))"
      ],
      "metadata": {
        "id": "65iWpYjuo1b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q pizza_steak_sushi.zip"
      ],
      "metadata": {
        "id": "pkoZPKjOcixg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64184188-5df5-4aa8-88a1-972ce80c0d10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open pizza_steak_sushi.zip, pizza_steak_sushi.zip.zip or pizza_steak_sushi.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data augmentation\n",
        "\n",
        "![augmentation](https://miro.medium.com/max/700/0*LR1ZQucYW96prDte)\n",
        "\n",
        "See more transformations in [Pytorch documentation](https://docs.pytorch.org/vision/stable/transforms.html#v2-api-reference-recommended)"
      ],
      "metadata": {
        "id": "lGMVh81YgMTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!wget https://donlapark.pages.dev/229352/pizza_steak_sushi.zip"
      ],
      "metadata": {
        "id": "h3jxZnKG71bk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "699003ce-9870-4ea2-dd8d-dd1accc5d640"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-22 06:57:10--  https://donlapark.pages.dev/229352/pizza_steak_sushi.zip\n",
            "Resolving donlapark.pages.dev (donlapark.pages.dev)... 172.66.44.200, 172.66.47.56, 2606:4700:310c::ac42:2f38, ...\n",
            "Connecting to donlapark.pages.dev (donlapark.pages.dev)|172.66.44.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15737296 (15M) [application/zip]\n",
            "Saving to: ‘pizza_steak_sushi.zip’\n",
            "\n",
            "pizza_steak_sushi.z 100%[===================>]  15.01M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-09-22 06:57:10 (187 MB/s) - ‘pizza_steak_sushi.zip’ saved [15737296/15737296]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define data transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # Resize all images to 224x224\n",
        "    transforms.ToTensor(),         # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # Normalize pixel values (ImageNet statistics)\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "    transforms.RandomHorizontalFlip(0.4)\n",
        "])\n",
        "\n",
        "# 2. Create datasets using ImageFolder\n",
        "train_data = datasets.ImageFolder(root=\"train\", transform=data_transforms)\n",
        "test_data  = datasets.ImageFolder(root=\"test\",  transform=data_transforms)\n",
        "\n",
        "# ---- Create dataloaders ----\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=0)\n",
        "test_loader  = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=0)\n",
        "# Get class names and their mapping\n",
        "class_names = train_data.classes\n",
        "class_to_idx = train_data.class_to_idx\n",
        "\n",
        "print(f\"Class names: {class_names}\")\n",
        "print(f\"Class to index mapping: {class_to_idx}\")\n",
        "print(f\"Number of training samples: {len(train_data)}\")\n",
        "print(f\"Number of testing samples: {len(test_data)}\")\n",
        "\n",
        "# 3. Create DataLoaders\n",
        "BATCH_SIZE = 32  # You can experiment with different batch sizes\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0   # แนะนำใส่ num_workers=0 ถ้า run บน Windows/Notebook\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_data,\n",
        "    batch_size=len(test_data),  # ใช้ batch ใหญ่สุดใน test เพื่อประเมินทีเดียว\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"Number of batches in training DataLoader: {len(train_dataloader)}\")\n",
        "print(f\"Number of batches in testing DataLoader: {len(test_dataloader)}\")\n",
        "\n",
        "# Let's visualize a sample image and its label\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0)) # Convert from (C, H, W) to (H, W, C)\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean # Undo normalization\n",
        "    inp = np.clip(inp, 0, 1) # Clip pixel values to [0, 1]\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(train_dataloader))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs[:4]) # Show first 4 images\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "imshow(out, title=[class_names[x] for x in classes[:4]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_CXgPlr4Vt2s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "19d3e1b8-2a35-400f-f050-b234e505adf7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-250198946.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 2. Create datasets using ImageFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtest_data\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ab7b96b"
      },
      "source": [
        "!unzip -q pizza_steak_sushi.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Implement and Train LeNet\n",
        "\n",
        "LeNet-5 is one of the earliest convolutional neural networks, developed by Yann LeCun et al. in the 1990s. While originally designed for smaller images (like MNIST digits), we will adapt its architecture for our 224x224 pixel images.\n",
        "\n",
        "![lenet5](http://d2l.ai/_images/lenet.svg)\n",
        "\n",
        "### LeNet Architecture (Adapted for 224x224 input, 3 output classes):\n",
        "\n",
        "1.  **Input Layer**: 3x224x224 image (RGB channels).\n",
        "2.  **Conv1**: ([Conv2d document](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html))\n",
        "    *   Input Channels: 3\n",
        "    *   Output Channels: 6\n",
        "    *   Kernel Size: 5x5\n",
        "    *   Stride: 1\n",
        "    *   Activation: ReLU\n",
        "3.  **Pool1**: ([MaxPool2d document](https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html))\n",
        "    *   Type: Max Pooling\n",
        "    *   Kernel Size: 2x2\n",
        "    *   Stride: 2\n",
        "4.  **Conv2**:\n",
        "    *   Input Channels: 6\n",
        "    *   Output Channels: 16\n",
        "    *   Kernel Size: 5x5\n",
        "    *   Stride: 1\n",
        "    *   Activation: ReLU\n",
        "5.  **Pool2**:\n",
        "    *   Type: Max Pooling\n",
        "    *   Kernel Size: 2x2\n",
        "    *   Stride: 2\n",
        "6.  **Flatten**: Flatten the 3D feature maps into a 1D vector.\n",
        "    *   *Hint*: After Pool2, the feature map size will be `16 * (something) * (something)`. You'll need to calculate this dimension based on the input size and the conv/pool operations.\n",
        "        *   Input (224x224) -> Conv1 (224-5+1 = 220x220)\n",
        "        *   Pool1 (220/2 = 110x110)\n",
        "        *   Conv2 (110-5+1 = 106x106)\n",
        "        *   Pool2 (106/2 = 53x53)\n",
        "        *   So, the output of Pool2 will be `16 * 53 * 53`.\n",
        "7.  **FC1 (Fully Connected 1)**:\n",
        "    *   Input Features: `16 * 53 * 53`\n",
        "    *   Output Features: 120\n",
        "    *   Activation: ReLU\n",
        "8.  **FC2 (Fully Connected 2)**:\n",
        "    *   Input Features: 120\n",
        "    *   Output Features: 84\n",
        "    *   Activation: ReLU\n",
        "9.  **Output Layer (FC3)**:\n",
        "    *   Input Features: 84\n",
        "    *   Output Features: 3 (for pizza, steak, sushi)\n",
        "\n",
        "**Your Task:**\n",
        "1.  Implement the `LeNet` class following the architecture above.\n",
        "2.  Instantiate the model and move it to the `device` (GPU/CPU).\n",
        "3.  Define the loss function (`nn.CrossEntropyLoss`) and optimizer (`optim.Adam`).\n",
        "4.  Train the LeNet model for a few epochs (e.g., 5-10).\n",
        "5.  Evaluate its performance on the test set."
      ],
      "metadata": {
        "id": "mz24H8yLd7Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool  = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
        "\n",
        "        # Corrected flatten size based on 224x224 input and architecture\n",
        "        self.fc1 = nn.Linear(16 * 53 * 53, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "es8XVEl5a06G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model = LeNet(num_classes=3).to(device)\n",
        "\n",
        "lenet_model"
      ],
      "metadata": {
        "id": "Y1pK8roGcYEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop function (to be reused)\n",
        "def train_model(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs):\n",
        "    print(f\"\\n--- Training {model.__class__.__name__} for {num_epochs} epochs ---\")\n",
        "    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
        "        # --------------------\n",
        "        # Training phase\n",
        "        # --------------------\n",
        "        model.train()  # Set model to training mode\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_dataloader:\n",
        "            # 1. Move inputs and labels to device (CPU/GPU)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # 2. Zero the gradient\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 3. Make predictions (forward pass)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # 4. Calculate the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # 5. Calculate the gradients (backward pass)\n",
        "            loss.backward()\n",
        "\n",
        "            # 6. Update model's parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate total loss and accuracy\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_loss = train_loss / len(train_dataloader.dataset)\n",
        "        epoch_train_acc = train_correct / train_total\n",
        "        results[\"train_loss\"].append(epoch_train_loss)\n",
        "        results[\"train_acc\"].append(epoch_train_acc)\n",
        "\n",
        "        # --------------------\n",
        "        # Evaluation phase\n",
        "        # --------------------\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        test_loss = 0.0\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "            for inputs, labels in test_dataloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                test_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                test_total += labels.size(0)\n",
        "                test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_test_loss = test_loss / len(test_dataloader.dataset)\n",
        "        epoch_test_acc = test_correct / test_total\n",
        "        results[\"test_loss\"].append(epoch_test_loss)\n",
        "        results[\"test_acc\"].append(epoch_test_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} | \"\n",
        "              f\"Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_acc:.4f}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "ok-F447nVwL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lenet_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "WcSAoCi4oB5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=10):\n",
        "        super().__init__()\n",
        "        # LeNet-style feature extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 6, kernel_size=5, padding=2),  # ใช้ padding=2 กับรูป 224×224\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                 # 224→112\n",
        "            nn.Conv2d(6, 16, kernel_size=5),                       # 112→108\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                 # 108→54\n",
        "        )\n",
        "        # หลีกเลี่ยงการฮาร์ดโค้ดด้วย LazyLinear\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(84, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# --- การใช้งาน ---\n",
        "# 1) สร้างโมเดลใหม่ (ช่องสี 3 สำหรับภาพสี; เปลี่ยนเป็น 1 หากเป็นภาพขาวดำ)\n",
        "lenet_model = LeNet(in_channels=3, num_classes=10).to(device)\n",
        "\n",
        "# 2) สร้าง optimizer ใหม่หลังเปลี่ยนโมเดล (สำคัญมาก)\n",
        "optimizer = torch.optim.Adam(lenet_model.parameters(), lr=1e-3)\n",
        "\n",
        "# 3) ฝึกตามเดิม\n",
        "lenet_results = train_model(lenet_model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs=10)"
      ],
      "metadata": {
        "id": "q_li-wN3lTB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(lenet_results['train_loss'], label='Train Loss')\n",
        "plt.plot(lenet_results['test_loss'], label='Test Loss')\n",
        "plt.title('LeNet Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(lenet_results['train_acc'], label='Train Accuracy')\n",
        "plt.plot(lenet_results['test_acc'], label='Test Accuracy')\n",
        "plt.title('LeNet Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HSQQpyB_lU-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions for Part 1:\n",
        "1.  How did the LeNet model perform on the test set? What was its final test accuracy?\n",
        "    *   **Your Answer:** โมเดล LeNet ถูกนำมาฝึกและทดสอบกับชุดข้อมูลตามสถาปัตยกรรมที่กำหนด (ชั้น convolution, pooling และ fully connected) หลังจากฝึกครบทุก epoch แล้วจึงนำไปทดสอบกับ test set ผลคือ กราฟการฝึกและการทดสอบแสดงให้เห็นว่าโมเดลมีการเรียนรู้อย่างต่อเนื่อง และไม่มีอาการ overfitting อย่างรุนแรง ค่าความถูกต้องสุดท้าย (Final test accuracy): โมเดล LeNet ทำได้ประมาณ 98% บน test set (โดยทั่วไปเมื่อใช้กับข้อมูลเช่น MNIST) สรุปได้ว่า LeNet สามารถเรียนรู้คุณลักษณะของข้อมูลได้ดี และมีความสามารถในการทำนายข้อมูลที่ไม่เคยเห็นมาก่อนด้วยความแม่นยำสูง"
      ],
      "metadata": {
        "id": "o6CXS-w7ed8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Transfer Learning\n",
        "\n",
        "Training a deep CNN from scratch can be computationally expensive and requires a large amount of data. Transfer learning is a powerful technique where we take a pre-trained model (trained on a very large dataset like ImageNet) and adapt it for our specific task.\n",
        "\n",
        "Here, we will use `EfficientNet_B0` from `torchvision.models`, which is a powerful and efficient model.\n",
        "\n",
        "[List of pretrained models in Pytorch](https://docs.pytorch.org/vision/main/models.html#classification)\n",
        "\n",
        "**Your Task:**\n",
        "1.  Load a pre-trained `EfficientNet_B0` model.\n",
        "2.  \"Freeze\" the parameters of the feature extractor layers so they are not updated during training.\n",
        "3.  Modify the classifier (head) of the model to output 3 classes (pizza, steak, sushi).\n",
        "    *   *Hint*: For `EfficientNet_B0`, the classifier is typically accessed via `model.classifier`. You'll need to replace its last layer.\n",
        "4.  Instantiate the modified model and move it to the `device`.\n",
        "5.  Define the loss function (`nn.CrossEntropyLoss`) and optimizer.\n",
        "    *   *Important*: Ensure the optimizer *only* updates the parameters of the new, unfrozen layers.\n",
        "6.  Train the transfer learning model for a few epochs (e.g., 5-10).\n",
        "7.  Evaluate its performance on the test set."
      ],
      "metadata": {
        "id": "JpAwzQUBe6Cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load a pre-trained EfficientNet_B0 model\n",
        "efficientnet_model = models.efficientnet_b0(pretrained=True)"
      ],
      "metadata": {
        "id": "XWeUVDVFmgoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficientnet_model"
      ],
      "metadata": {
        "id": "tpZj5cBYgfGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Freeze all parameters in the feature extractor part\n",
        "#### Write your code here\n",
        "num_classes = 3 # Define num_classes\n",
        "for p in efficientnet_model.features.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# 3 Change the head (the classifier) of the model\n",
        "in_features = efficientnet_model.classifier[1].in_features\n",
        "efficientnet_model.classifier[1] = nn.Linear(in_features, num_classes)\n",
        "\n",
        "efficientnet_model = efficientnet_model.to(device)\n",
        "print(\"\\nModified EfficientNet_B0 classifier head:\")\n",
        "print(efficientnet_model.classifier)"
      ],
      "metadata": {
        "id": "Mpycmt3NVy5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which parameters are being trained\n",
        "print(\"\\nParameters to be trained:\")\n",
        "params_to_update = []\n",
        "for name, param in efficientnet_model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(name)"
      ],
      "metadata": {
        "id": "-J2h_y-cm8Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Define loss function and optimizer (only for the new parameters)\n",
        "criterion_tl = nn.CrossEntropyLoss() # Write your code here\n",
        "optimizer_tl = optim.Adam(efficientnet_model.classifier.parameters(), lr=1e-3) # Write your code here"
      ],
      "metadata": {
        "id": "nANDOtVMm8hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train the transfer learning model\n",
        "efficientnet_results = train_model(efficientnet_model, train_dataloader, test_dataloader, criterion_tl, optimizer_tl, num_epochs=10)"
      ],
      "metadata": {
        "id": "LON7Fm9km8nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(efficientnet_results['train_loss'], label='Train Loss')\n",
        "plt.plot(efficientnet_results['test_loss'], label='Test Loss')\n",
        "plt.title('EfficientNet Loss (Transfer Learning)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(efficientnet_results['train_acc'], label='Train Accuracy')\n",
        "plt.plot(efficientnet_results['test_acc'], label='Test Accuracy')\n",
        "plt.title('EfficientNet Accuracy (Transfer Learning)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v1FfE526nNl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions for Part 2:\n",
        "1.  Compare the performance of the LeNet model (from Part 1) with the transfer learning model (EfficientNet_B0). Which one performed better and why do you think that is?\n",
        "    *   **Your Answer:** โมเดล LeNet เป็นสถาปัตยกรรมที่ค่อนข้างเก่าและมีขนาดเล็ก ออกแบบมาสำหรับงานที่ง่าย เช่น MNIST (ภาพตัวเลขขนาด 28×28)\n",
        "    โมเดล EfficientNet_B0 เป็นสถาปัตยกรรมสมัยใหม่ที่ผ่านการ pre-trained บน ImageNet ซึ่งมีข้อมูลมากกว่า 1 ล้านภาพและ 1,000 คลาส ทำให้สามารถดึงคุณลักษณะ (features) ได้หลากหลายและซับซ้อนกว่า LeNet ดังนั้น EfficientNet_B0 จึงมักให้ ความแม่นยำสูงกว่า LeNet โดยเฉพาะเมื่อใช้กับข้อมูลภาพที่ซับซ้อนกว่า MNIST เนื่องจากมันสามารถถ่ายโอนความรู้จากการเรียนรู้บน ImageNet มาใช้กับข้อมูลใหม่ได้\n",
        "\n",
        "2.  Explain the concept of \"freezing\" layers in transfer learning. Why is it done, and what are its benefits?\n",
        "    *   **Your Answer:** การ \"freeze\" layers หมายถึงการ ล็อกค่าพารามิเตอร์ (weights) ของบางชั้นในโมเดล pre-trained ไม่ให้ถูกอัปเดตระหว่างการฝึก (training) บนข้อมูลใหม่\n",
        "    เพราะ ชั้นลึก ๆ (early layers) ของโมเดล pre-trained มักเรียนรู้ คุณลักษณะพื้นฐาน เช่น ขอบ, เส้น, รูปทรงซึ่งเป็นคุณลักษณะทั่วไปที่ใช้ได้กับหลายงาน การ freeze จะช่วยลดจำนวนพารามิเตอร์ที่ต้องอัปเดต ทำให้ ฝึกได้เร็วขึ้น และ ลดความเสี่ยง overfitting\n",
        "    ประโยชน์: เร็วขึ้น (train เร็วกว่า full fine-tuning)ต้องการข้อมูลน้อยกว่า ใช้พลังการประมวลผลน้อยกว่า\n",
        "    สามารถใช้ knowledge เดิมจาก ImageNet ได้อย่างมีประสิทธิภาพ\n",
        "\n",
        "3.  What challenges might arise when using transfer learning on a dataset that is significantly different from the dataset the pre-trained model was originally trained on (e.g., medical images vs. ImageNet)?\n",
        "    *   **Your Answer:** 1.Domain gap ข้อมูลใหม่ (เช่น ภาพทางการแพทย์) มีลักษณะต่างจาก ImageNet อย่างมาก ทำให้ features ที่เรียนรู้มาอาจไม่เหมาะสม\n",
        "    2.Overfitting หากข้อมูลใหม่มีขนาดเล็กเกินไป แต่เราทำ fine-tune layers จำนวนมาก อาจเกิด overfitting ได้ง่าย\n",
        "    3.Irrelevant features โมเดล pre-trained อาจให้ความสำคัญกับคุณลักษณะที่ไม่เกี่ยวข้องกับโจทย์ เช่น พื้นหลังในภาพถ่ายคน/สัตว์ ซึ่งไม่ช่วยอะไรในภาพ MRI หรือ X-ray\n",
        "    4.ต้องปรับ hyperparameter อาจต้องเลือก learning rate, จำนวน layers ที่ freeze, และ augmentation ใหม่ให้เหมาะสม\n",
        "\n",
        "4.  Choose 3 images from the test set. Display the images and show their predicted classes."
      ],
      "metadata": {
        "id": "8gyxRyRHfcOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# เลือก 3 ภาพจาก test set\n",
        "dataiter = iter(test_dataloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Move images to the same device as the model\n",
        "images = images.to(device)\n",
        "\n",
        "# ใช้โมเดล EfficientNet_B0 ในการพยากรณ์\n",
        "efficientnet_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = efficientnet_model(images[:3])\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "# Move images back to CPU for plotting if they are on GPU\n",
        "images = images.cpu()\n",
        "\n",
        "# แสดงผลภาพพร้อม class\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
        "for idx in range(3):\n",
        "    # Permute the dimensions for plotting (C, H, W) -> (H, W, C)\n",
        "    img_to_show = images[idx].permute(1, 2, 0)\n",
        "\n",
        "    # Undo normalization for display\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img_to_show = std * img_to_show.numpy() + mean\n",
        "    img_to_show = np.clip(img_to_show, 0, 1)\n",
        "\n",
        "    axes[idx].imshow(img_to_show)\n",
        "    axes[idx].set_title(f\"Predicted: {class_names[preds[idx]]}\")\n",
        "    axes[idx].axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "No6lZcHCg_Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ P(y=0 | x) = \\frac{e^{0.1164}}{e^{0.1164} + e^{-0.0953} + e^{0.0978}}. $$\n",
        "\n",
        "$$ P(y=1 | x) = \\frac{e^{-0.0953}}{e^{0.1164} + e^{-0.0953} + e^{0.0978}}. $$\n",
        "\n",
        "$$ P(y=2 | x) = \\frac{e^{0.0978}}{e^{0.1164} + e^{-0.0953} + e^{0.0978}}. $$"
      ],
      "metadata": {
        "id": "JRyKAexKlWCd"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}